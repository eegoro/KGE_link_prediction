{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data/info.txt', 'r') as info_file:\n",
    "    NEG_SAMPLE_SIZE = json.load(info_file)['NEG_SAMPLE_SIZE']\n",
    "NEG_SAMPLE_SIZE = 100\n",
    "BATCH_SIZE_TEST = 1024\n",
    "NUM_WORKERS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ENTITY: 21431\n",
      "NUMBER OF RELETION: 1\n",
      "\n",
      "TRAIN: 58%\n",
      "VALID: 17%\n",
      "TEST: 25%\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv (r'data/train_100_1.csv')\n",
    "test = pd.read_csv (r'data/test_100_1.csv')\n",
    "valid = pd.read_csv (r'data/valid_100_1.csv')\n",
    "\n",
    "for i in [train, valid, test]:\n",
    "    i['neg_head'] = [eval(l) for l in i['neg_head']]\n",
    "    i['neg_tail'] = [eval(l) for l in i['neg_tail']]\n",
    "\n",
    "nentity = len(pd.concat([train['head'], valid['head'], test['head']]).unique()) + len(pd.concat([train['tail'], valid['tail'], test['tail']]).unique())\n",
    "nrelation = len(pd.concat([train['relation'], valid['relation'], test['relation']]).unique())\n",
    "\n",
    "print(f'NUMBER OF ENTITY: {nentity}')\n",
    "print(f'NUMBER OF RELETION: {nrelation}')\n",
    "print()\n",
    "print(f'TRAIN: {round(len(train)/(len(train)+len(test)+len(valid))*100)}%')\n",
    "print(f'VALID: {round(len(valid)/(len(train)+len(test)+len(valid))*100)}%')\n",
    "print(f'TEST: {round(len(test)/(len(train)+len(test)+len(valid))*100)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGEDataset(Dataset):\n",
    "\n",
    "  def __init__(self,table,mode='train'):\n",
    "\n",
    "    self.mode = mode\n",
    "    self.head = torch.tensor(np.array(table['head']))\n",
    "    self.tail = torch.tensor(np.array(table['tail']))\n",
    "    self.relation = torch.tensor(np.array(table['relation']))\n",
    "    self.neg_head = torch.tensor(np.array(list(table['neg_head'])))\n",
    "    self.neg_tail = torch.tensor(np.array(list(table['neg_tail'])))\n",
    "\n",
    "    if mode=='train':\n",
    "      self.subsampling_weight = torch.tensor(np.array(table['subsampling_weight']))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.head)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    if self.mode == 'train':\n",
    "      return self.head[idx],self.tail[idx],self.relation[idx], self.neg_head[idx],self.neg_tail[idx], self.subsampling_weight[idx]\n",
    "    else:\n",
    "      return self.head[idx],self.tail[idx],self.relation[idx], self.neg_head[idx],self.neg_tail[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def eval(self, input_dict):\n",
    "        y_pred_pos, y_pred_neg = input_dict['y_pred_pos'], input_dict['y_pred_neg']\n",
    "        y_pred = torch.cat([y_pred_pos.view(-1,1), y_pred_neg], dim = 1)\n",
    "        argsort = torch.argsort(y_pred, dim = 1, descending = True)\n",
    "        ranking_list = torch.nonzero(argsort == 0, as_tuple=False)\n",
    "        ranking_list = ranking_list[:, 1] + 1\n",
    "        hits1_list = (ranking_list <= 1).to(torch.float)\n",
    "        hits3_list = (ranking_list <= 3).to(torch.float)\n",
    "        hits10_list = (ranking_list <= 10).to(torch.float)\n",
    "        mrr_list = 1./ranking_list.to(torch.float)\n",
    "\n",
    "        return mrr_list, hits1_list, hits3_list, hits10_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Random Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEG_SAMPLE_SIZE': 100, 'MRR': 0.054304301738739014, 'HITS@1': 0.013236735947430134, 'HITS@3': 0.03247664123773575, 'HITS@10': 0.10075974464416504}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "metrics = []\n",
    "test_loader=DataLoader(KGEDataset(test,mode='test'),batch_size=BATCH_SIZE_TEST,shuffle=True,num_workers=NUM_WORKERS)\n",
    "for test_values in test_loader:\n",
    "        positive_score = torch.randint(1, 100, (BATCH_SIZE_TEST,1))\n",
    "        negative_score = torch.randint(1, 100, (BATCH_SIZE_TEST, NEG_SAMPLE_SIZE))\n",
    "\n",
    "        metrics.append([metric.mean() for metric in evaluator.eval({'y_pred_pos': positive_score, 'y_pred_neg': negative_score})])\n",
    "\n",
    "metrics = torch.tensor(metrics).mean(0)\n",
    "dict_metrics_rand = {'NEG_SAMPLE_SIZE': NEG_SAMPLE_SIZE,\n",
    "                'MRR': float(metrics[0]),\n",
    "                'HITS@1': float(metrics[1]),\n",
    "                'HITS@3': float(metrics[2]),\n",
    "                'HITS@10': float(metrics[3])}\n",
    "\n",
    "with open('baseline//metrics_random.txt', 'w') as var_file:\n",
    "            json.dump(dict_metrics_rand, var_file)\n",
    "\n",
    "print(dict_metrics_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [19:29<00:00,  1.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEG_SAMPLE_SIZE': 100, 'MRR': 0.21032452583312988, 'HITS@1': 0.1024353951215744, 'HITS@3': 0.2185317426919937, 'HITS@10': 0.44210225343704224}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "metrics = []\n",
    "freq = pd.concat([train['tail']]).value_counts()\n",
    "freq = freq.add(pd.Series([0]*len(set(pd.concat([test['tail'], valid['tail']]))-set(train['tail'])), index = list(set(pd.concat([test['tail'], valid['tail']]))-set(train['tail']))), fill_value=0)\n",
    "# freq = pd.concat([train['head'], valid['head'], test['head'], train['tail'], valid['tail'], test['tail']]).value_counts()\n",
    "\n",
    "test_loader=DataLoader(KGEDataset(test,mode='test'),batch_size=BATCH_SIZE_TEST,shuffle=True,num_workers=NUM_WORKERS)\n",
    "for test_values in tqdm(test_loader):\n",
    "        \n",
    "        head,tail,relation,neg_head,neg_tail = test_values\n",
    "\n",
    "        positive_score = torch.tensor(list(freq[list(tail)]))\n",
    "        # negative_head_score = torch.tensor(list(freq[list(neg_head.view(-1))])).view(-1,NEG_SAMPLE_SIZE)\n",
    "        negative_tail_score = torch.tensor(list(freq[list(neg_tail.view(-1))])).view(-1,NEG_SAMPLE_SIZE)\n",
    "\n",
    "        metrics.append([metric.mean() for metric in evaluator.eval({'y_pred_pos': positive_score, 'y_pred_neg': negative_tail_score})])\n",
    "\n",
    "metrics = torch.tensor(metrics).mean(0)\n",
    "dict_metrics_freq = {'NEG_SAMPLE_SIZE': NEG_SAMPLE_SIZE,\n",
    "                'MRR': float(metrics[0]),\n",
    "                'HITS@1': float(metrics[1]),\n",
    "                'HITS@3': float(metrics[2]),\n",
    "                'HITS@10': float(metrics[3])}\n",
    "\n",
    "with open('baseline//metrics_frequency.txt', 'w') as var_file:\n",
    "            json.dump(dict_metrics_freq, var_file)\n",
    "\n",
    "print(dict_metrics_freq)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c688d49920be7bbf78ae94a9bca1b2bf919231a3254bde77120844e21401b4fe"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ml]",
   "language": "python",
   "name": "conda-env-.conda-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
