{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_SAMPLE_SIZE = 10\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "VALIDATION_INTERVAL = 100\n",
    "SAVE_ITERATION = 100\n",
    "EMBEDDING_SIZE = 10\n",
    "GAMMA = 12\n",
    "LEARNING_RATE = 0.0001\n",
    "REGULARIZATION = 0\n",
    "ADVERSARIAL_TEMPERATURE  = 1 #In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
    "NUM_WORKERS = 6\n",
    "\n",
    "double_entity_embedding = True # True if RotatE, ComplEx\n",
    "double_relation_embedding = True # True if ComplEx\n",
    "uni_weight = True\n",
    "\n",
    "NAME_PATH = 'log//'+ time.ctime()\n",
    "os.mkdir(NAME_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {'NEG_SAMPLE_SIZE': NEG_SAMPLE_SIZE,\n",
    "             'EPOCH': EPOCH,\n",
    "             'BATCH_SIZE': BATCH_SIZE,\n",
    "             'BATCH_SIZE_TEST': BATCH_SIZE_TEST,\n",
    "             'VALIDATION_INTERVAL': VALIDATION_INTERVAL,\n",
    "             'SAVE_ITERATION': SAVE_ITERATION,\n",
    "             'EMBEDDING_SIZE': EMBEDDING_SIZE,\n",
    "             'GAMMA':  GAMMA,\n",
    "             'LEARNING_RATE': LEARNING_RATE,\n",
    "             'REGULARIZATION': REGULARIZATION,\n",
    "             'ADVERSARIAL_TEMPERATURE': ADVERSARIAL_TEMPERATURE,\n",
    "             'NUM_WORKERS': NUM_WORKERS}\n",
    "\n",
    "with open(NAME_PATH+'//varibles.txt', 'w') as var_file:\n",
    "            json.dump(variables, var_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('clear_data_realbank.csv')\n",
    "df.drop('TRANS_DETAIL', inplace=True, axis=1)\n",
    "df.rename(columns = {'RETAILER' : 'tail', 'CustomerKey' : 'head',\n",
    "                        'MCC' : 'tail_type', 'AMOUNT_EQ' : 'relation'   }, \n",
    "                            inplace = True) \n",
    "# некоторые id магазинов имели разные категории. Здесь id c разл категорией приравниваются к -2 и затем удаляются\n",
    "for i in (range(min(df.tail_type.unique()),max(df.tail_type.unique())+1)):\n",
    "    for j in range(i+1,max(df.tail_type.unique())+1):\n",
    "        for k in (set(df[df.tail_type==i]['tail'].unique())&set(df[df.tail_type==j]['tail'].unique())):\n",
    "            df.loc[df['tail'] == k,'tail_type'] = -2\n",
    "df = df[df.tail_type != -2].reset_index(drop=True)\n",
    "\n",
    "# траты разбиваются на 8 категорий и в дальнейшем будут характеризировать отношения\n",
    "df.relation = pd.qcut(df.relation, q=8, \n",
    "        labels=[\"small\", \"medium_small\", \"medium_small_2\", 'medium_1', 'medium_2', 'medium_large_2', 'medium_large', 'large'])\n",
    "\n",
    "# удаление непопулярных категорий магазинов\n",
    "top_mcc = list(df.tail_type.value_counts()[:10].rename_axis('unique_values').reset_index(name='counts')['unique_values'])\n",
    "df = df[df.tail_type.isin(top_mcc)].reset_index(drop=True)\n",
    "df = df.sort_values('tail_type')\n",
    "\n",
    "# присвоение уникальным id пользователей и магазинов чисел от 0 до len(уникальных id)\n",
    "df['head'] = pd.factorize(df['head'])[0]\n",
    "max_person = max(df['head'])\n",
    "df['tail'] = pd.factorize(df['tail'])[0] + max_person + 1\n",
    "\n",
    "# словарь с типами сущностей и списком их id\n",
    "dict_id = {}\n",
    "dict_id['person'] = list(range(0,max(df['head'])+1))\n",
    "for tail_type in df['tail_type'].unique():\n",
    "    dict_id[tail_type] = list(range(min(df[df.tail_type==tail_type]['tail']),(max(df[df.tail_type==tail_type]['tail'])+1)))\n",
    "\n",
    "# присвоение чисел отношениям\n",
    "dict_rel = {key: idx for idx,key in enumerate(pd.factorize(df.relation)[1].categories)}\n",
    "df['relation'] = df['relation'].apply(lambda x: dict_rel[x])\n",
    "\n",
    "# создание негативных сущностей \n",
    "# neg_head --> tail      neg_tail --> head\n",
    "df['neg_head'] = [random.sample(dict_id['person'],NEG_SAMPLE_SIZE) for _ in range(len(df))]\n",
    "df['neg_tail'] = [random.sample(dict_id[i], NEG_SAMPLE_SIZE) for i in df.tail_type]\n",
    "\n",
    "# разбиение на выборки по времени\n",
    "train = df[(df.tstmp >= '2018-01-01 00:00:00+03:00') & (df.tstmp < '2018-08-01 00:00:00+03:00')]\n",
    "train.drop('tstmp', inplace=True, axis=1, errors='ignore')\n",
    "valid = df[(df.tstmp >= '2018-08-01 00:00:00+03:00') & (df.tstmp < '2018-10-01 00:00:00+03:00')]\n",
    "valid.drop('tstmp', inplace=True, axis=1, errors='ignore')\n",
    "test = df[(df.tstmp >= '2018-10-01 00:00:00+03:00') & (df.tstmp < '2019-01-01 00:00:00+03:00')]\n",
    "test.drop('tstmp', inplace=True, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 58%\n",
      "VALID: 17%\n",
      "TEST: 25%\n"
     ]
    }
   ],
   "source": [
    "print(f'TRAIN: {round(len(train)/len(df)*100)}%')\n",
    "print(f'VALID: {round(len(valid)/len(df)*100)}%')\n",
    "print(f'TEST: {round(len(test)/len(df)*100)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1795880/1795880 [00:58<00:00, 30620.96it/s]\n"
     ]
    }
   ],
   "source": [
    "train_count, train_true_head, train_true_tail = defaultdict(lambda: 4), defaultdict(list), defaultdict(list)\n",
    "for i in tqdm(train.index):\n",
    "    head, relation, tail = train.loc[i,'head'], train.loc[i,'relation'],  train.loc[i,'tail']\n",
    "    train_count[(head, relation)] += 1\n",
    "    train_count[(tail, -relation-1)] += 1\n",
    "    train_true_head[(relation, tail)].append(head)\n",
    "    train_true_tail[(head, relation)].append(tail)\n",
    "\n",
    "train['subsampling_weight'] = [(1/(train_count[(train.loc[i,'head'], train.loc[i,'relation'])] \n",
    "                                + train_count[(train.loc[i,'tail'], -train.loc[i,'relation']-1)]))**(1/2)\n",
    "                                                                                for i in train.index]\n",
    "\n",
    "nentity = len(df['tail'].unique())+len(df['head'].unique())\n",
    "nrelation = len(df['relation'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGEModel(nn.Module):\n",
    "    def __init__(self, nentity, nrelation, embedding_size, gamma, evaluator,\n",
    "                 double_entity_embedding=False, double_relation_embedding=False, epsilon = 2.0):\n",
    "        super(KGEModel, self).__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(\n",
    "            torch.Tensor([gamma]), \n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        self.embedding_range = nn.Parameter(\n",
    "            torch.Tensor([(self.gamma.item() + epsilon) / embedding_size]), \n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        self.entity_dim = embedding_size*2 if double_entity_embedding else embedding_size\n",
    "        self.relation_dim = embedding_size*2 if double_relation_embedding else embedding_size\n",
    "        \n",
    "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.entity_embedding, \n",
    "            a=-self.embedding_range.item(), \n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "        \n",
    "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.relation_embedding, \n",
    "            a=-self.embedding_range.item(), \n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "\n",
    "        self.evaluator = evaluator\n",
    "        \n",
    "    def forward(self, head, tail, relation, neg_head, neg_tail):\n",
    "        head_E = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=head\n",
    "            ).unsqueeze(1)\n",
    "        \n",
    "        relation_E = torch.index_select(\n",
    "                self.relation_embedding, \n",
    "                dim=0, \n",
    "                index=relation\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "        tail_E = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=tail\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "        neg_head_E = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=neg_head.view(-1)\n",
    "            ).view(neg_head.size(0), neg_head.size(1), -1)  #batch_size, negative_sample_size\n",
    "            \n",
    "        neg_tail_E  = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=neg_tail.view(-1)\n",
    "            ).view(neg_tail.size(0), neg_tail.size(1), -1) \n",
    "            \n",
    "        #TransE\n",
    "        positive_score = (head_E + relation_E) - tail_E\n",
    "        negative_tail_score = (head_E + relation_E) - neg_tail_E\n",
    "        negative_head_score = neg_head_E + (relation_E - tail_E)\n",
    "\n",
    "        positive_score = self.gamma.item() - torch.norm(positive_score, p=1, dim=2)\n",
    "        negative_tail_score = self.gamma.item() - torch.norm(negative_tail_score, p=1, dim=2)\n",
    "        negative_head_score = self.gamma.item() - torch.norm(negative_head_score, p=1, dim=2)\n",
    "        \n",
    "        return positive_score, negative_tail_score, negative_head_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def eval(self, input_dict):\n",
    "        y_pred_pos, y_pred_neg = input_dict['y_pred_pos'], input_dict['y_pred_neg']\n",
    "        y_pred = torch.cat([y_pred_pos.view(-1,1), y_pred_neg], dim = 1)\n",
    "        argsort = torch.argsort(y_pred, dim = 1, descending = True)\n",
    "        ranking_list = torch.nonzero(argsort == 0, as_tuple=False)\n",
    "        ranking_list = ranking_list[:, 1] + 1\n",
    "        hits1_list = (ranking_list <= 1).to(torch.float)\n",
    "        hits3_list = (ranking_list <= 3).to(torch.float)\n",
    "        hits10_list = (ranking_list <= 10).to(torch.float)\n",
    "        mrr_list = 1./ranking_list.to(torch.float)\n",
    "\n",
    "        return mrr_list, hits1_list, hits3_list, hits10_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self,table,mode='train'):\n",
    "\n",
    "    self.mode = mode\n",
    "    self.head = torch.tensor(np.array(table['head']))\n",
    "    self.tail = torch.tensor(np.array(table['tail']))\n",
    "    self.relation = torch.tensor(np.array(table['relation']))\n",
    "    self.neg_head = torch.tensor(np.array(list(table['neg_head'])))\n",
    "    self.neg_tail = torch.tensor(np.array(list(table['neg_tail'])))\n",
    "\n",
    "    if mode=='train':\n",
    "      self.subsampling_weight = torch.tensor(np.array(table['subsampling_weight']))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.head)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    if self.mode == 'train':\n",
    "      return self.head[idx],self.tail[idx],self.relation[idx], self.neg_head[idx],self.neg_tail[idx], self.subsampling_weight[idx]\n",
    "    else:\n",
    "      return self.head[idx],self.tail[idx],self.relation[idx], self.neg_head[idx],self.neg_tail[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "kge_model = KGEModel(\n",
    "        nentity=nentity,\n",
    "        nrelation=nrelation,\n",
    "        embedding_size=EMBEDDING_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        double_entity_embedding=double_entity_embedding,\n",
    "        double_relation_embedding=double_relation_embedding,\n",
    "        evaluator=evaluator\n",
    "    )\n",
    "kge_model = kge_model.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()), \n",
    "            lr=LEARNING_RATE\n",
    "        )\n",
    "\n",
    "log, mrr, hits1, hits3, hits10 = [], [], [], [], []\n",
    "\n",
    "for num_epoch in (range(EPOCH)):\n",
    "    train_loader=DataLoader(MyDataset(train),batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS)\n",
    "    iteration = math.ceil(len(train)/BATCH_SIZE) \n",
    "    for i in tqdm(range(iteration)):\n",
    "        kge_model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        head,tail,relation,neg_head,neg_tail,subsampling_weight = next(iter(train_loader))\n",
    "\n",
    "        head = head.cuda()\n",
    "        tail = tail.cuda()\n",
    "        relation = relation.cuda()\n",
    "        neg_head = neg_head.cuda()\n",
    "        neg_tail = neg_tail.cuda()\n",
    "        subsampling_weight = subsampling_weight.cuda()\n",
    "\n",
    "        positive_score, negative_tail_score, negative_head_score = kge_model(head, tail, relation, neg_head, neg_tail)\n",
    "\n",
    "        positive_score = F.logsigmoid(positive_score).squeeze(dim = 1)\n",
    "        if ADVERSARIAL_TEMPERATURE!=0.0:\n",
    "            #In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
    "            negative_tail_score = (F.softmax(negative_tail_score * ADVERSARIAL_TEMPERATURE, dim = 1).detach() \n",
    "                              * F.logsigmoid(-negative_tail_score)).sum(dim = 1)\n",
    "            negative_head_score = (F.softmax(negative_head_score * ADVERSARIAL_TEMPERATURE, dim = 1).detach() \n",
    "                              * F.logsigmoid(-negative_head_score)).sum(dim = 1)\n",
    "        else:\n",
    "            negative_tail_score = F.logsigmoid(-negative_tail_score).mean(dim = 1)\n",
    "            negative_head_score = F.logsigmoid(-negative_head_score).mean(dim = 1)\n",
    "\n",
    "        if uni_weight:\n",
    "            positive_sample_loss = - positive_score.mean()\n",
    "            negative_sample_tail_loss = - negative_tail_score.mean()\n",
    "            negative_sample_head_loss = - negative_head_score.mean()\n",
    "        else:\n",
    "            positive_sample_loss = - (subsampling_weight * positive_score).sum()/subsampling_weight.sum()\n",
    "            negative_sample_tail_loss = - (subsampling_weight * negative_tail_score).sum()/subsampling_weight.sum()\n",
    "            negative_sample_head_loss = - (subsampling_weight * negative_head_score).sum()/subsampling_weight.sum()\n",
    "\n",
    "        loss = (2*positive_sample_loss + negative_sample_tail_loss+negative_sample_head_loss)/4\n",
    "\n",
    "        if REGULARIZATION != 0.0:\n",
    "            #Use L3 regularization for ComplEx and DistMult\n",
    "            REGULARIZATION = REGULARIZATION * (\n",
    "                kge_model.entity_embedding.norm(p = 3)**3 + \n",
    "                kge_model.relation_embedding.norm(p = 3).norm(p = 3)**3\n",
    "            )\n",
    "            loss = loss + REGULARIZATION\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        log.append(float(loss))\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "# VALID\n",
    "        if (iteration+1)%VALIDATION_INTERVAL == 0:\n",
    "            valid_loader=DataLoader(MyDataset(valid,mode='test'),batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS)\n",
    "            iteration = math.ceil(len(valid)/BATCH_SIZE_TEST) \n",
    "            for i in range(iteration):\n",
    "                kge_model.eval()\n",
    "\n",
    "                head,tail,relation,neg_head,neg_tail = next(iter(valid_loader))\n",
    "\n",
    "                head = head.cuda()\n",
    "                tail = tail.cuda()\n",
    "                relation = relation.cuda()\n",
    "                neg_head = neg_head.cuda()\n",
    "                neg_tail = neg_tail.cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    positive_score, negative_tail_score, negative_head_score = kge_model(head, tail, relation, neg_head, neg_tail)\n",
    "\n",
    "                    mrr_tail, hits1_tail, hits3_tail, hits10_tail = kge_model.evaluator.eval({'y_pred_pos': positive_score, 'y_pred_neg': negative_tail_score})\n",
    "                    mrr_head, hits1_head, hits3_head, hits10_head = kge_model.evaluator.eval({'y_pred_pos': positive_score, 'y_pred_neg': negative_head_score})\n",
    "                    mrr_tail, hits1_tail, hits3_tail, hits10_tail = mrr_tail.mean(), hits1_tail.mean(), hits3_tail.mean(), hits10_tail.mean()\n",
    "                    mrr_head, hits1_head, hits3_head, hits10_head = mrr_head.mean(), hits1_head.mean(), hits3_head.mean(), hits10_head.mean()\n",
    "                    mrr.append(float((mrr_tail+mrr_head)/2))\n",
    "                    hits1.append(float((hits1_tail+hits1_head)/2))\n",
    "                    hits3.append(float((hits3_tail+hits3_head)/2))\n",
    "                    hits10.append(float((hits10_tail+hits10_head)/2))\n",
    "\n",
    "        if (num_epoch+1)%SAVE_ITERATION == 0:\n",
    "            with open(NAME_PATH+'//log.txt', 'w') as log_file, open(NAME_PATH+'//mrr.txt', 'w') as mrr_file:\n",
    "                json.dump(log, log_file)\n",
    "                json.dump(mrr, mrr_file)\n",
    "            plt.clf()\n",
    "            figure, axis = plt.subplots(2, 2)\n",
    "            axis[0,0].plot(log)\n",
    "            axis[0,0].set_title(\"TRAIN LOSS\")\n",
    "  \n",
    "            axis[0,1].plot(mrr)\n",
    "            axis[0,1].set_title(\"VALID MRR\")\n",
    "\n",
    "            axis[1,0].plot(hits1)\n",
    "            axis[1,0].set_title(\"VALID HITS@1\")\n",
    "\n",
    "            axis[1,1].plot(hits3)\n",
    "            axis[1,1].set_title(\"VALID HITS@1\")\n",
    "\n",
    "            figure.tight_layout()\n",
    "        \n",
    "            plt.savefig(NAME_PATH+'//result.png', dpi=300, facecolor='w', edgecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_test, hits1_test, hits3_test, hits10_test = [], [], [], []\n",
    "test_loader=DataLoader(MyDataset(test,mode='test'),batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS)\n",
    "iteration = math.ceil(len(test)/BATCH_SIZE_TEST) \n",
    "for i in range(iteration):\n",
    "        kge_model.eval()\n",
    "        \n",
    "        head,tail,relation,neg_head,neg_tail = next(iter(test_loader))\n",
    "\n",
    "        head = head.cuda()\n",
    "        tail = tail.cuda()\n",
    "        relation = relation.cuda()\n",
    "        neg_head = neg_head.cuda()\n",
    "        neg_tail = neg_tail.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            positive_score, negative_tail_score, negative_head_score = kge_model(head, tail, relation, neg_head, neg_tail)\n",
    "\n",
    "            mrr_tail, hits1_tail, hits3_tail, hits10_tail = kge_model.evaluator.eval({'y_pred_pos': positive_score, 'y_pred_neg': negative_tail_score})\n",
    "            mrr_head, hits1_head, hits3_head, hits10_head = kge_model.evaluator.eval({'y_pred_pos': positive_score, 'y_pred_neg': negative_head_score})\n",
    "            mrr_tail, hits1_tail, hits3_tail, hits10_tail = mrr_tail.mean(), hits1_tail.mean(), hits3_tail.mean(), hits10_tail.mean()\n",
    "            mrr_head, hits1_head, hits3_head, hits10_head = mrr_head.mean(), hits1_head.mean(), hits3_head.mean(), hits10_head.mean()\n",
    "            mrr_test.append(float((mrr_tail+mrr_head)/2))\n",
    "            hits1_test.append(float((hits1_tail+hits1_head)/2))\n",
    "            hits3_test.append(float((hits3_tail+hits3_head)/2))\n",
    "            hits10_test.append(float((hits10_tail+hits10_head)/2))\n",
    "\n",
    "dict_metrics = {'MRR': np.array(mrr_test).mean(),\n",
    "                'HITS@1': np.array(hits1_test).mean(),\n",
    "                'HITS@3': np.array(hits3_test).mean(),\n",
    "                'HITS@10': np.array(hits10_test).mean()}\n",
    "\n",
    "with open(NAME_PATH+'//metrics.txt', 'w') as var_file:\n",
    "            json.dump(dict_metrics, var_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Чтение\n",
    "# with open('log.txt', 'r') as log_file, open('mrr.txt', 'r') as mrr_file:\n",
    "#     log = json.load(log_file)\n",
    "#     mrr = json.load(mrr_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c688d49920be7bbf78ae94a9bca1b2bf919231a3254bde77120844e21401b4fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
