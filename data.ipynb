{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_SAMPLE_SIZE = 100\n",
    "info = {'NEG_SAMPLE_SIZE': NEG_SAMPLE_SIZE}\n",
    "\n",
    "if not (os.path.exists('data')):\n",
    "    os.mkdir('data')\n",
    "with open('data//info.txt', 'w') as info_file:\n",
    "            json.dump(info, info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv ('clear_data_realbank.csv')\n",
    "# direct = pd.read_csv('mapped_poitns.csv', sep='\\t', index_col=0)\n",
    "# direct['RETAILER'] = direct.field_1.str.split(' ').apply(lambda x:x[-1])\n",
    "# direct.drop('field_1', inplace=True, axis=1)\n",
    "# direct.drop('lat', inplace=True, axis=1)\n",
    "# direct.drop('lon', inplace=True, axis=1)\n",
    "# direct.drop('iter', inplace=True, axis=1)\n",
    "# df = pd.merge(df, direct, on = 'RETAILER', how='left')\n",
    "# df['district'] = df['district'].fillna('Другое')\n",
    "# df.head()\n",
    "# df.to_csv(r'clear_data_realbank.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('clear_data_realbank.csv')\n",
    "# df.drop('TRANS_DETAIL', inplace=True, axis=1)\n",
    "# df.rename(columns = {'RETAILER' : 'tail', 'CustomerKey' : 'head',\n",
    "#                         'MCC' : 'tail_type', 'AMOUNT_EQ' : 'relation', 'district': 'district'  }, \n",
    "#                             inplace = True) \n",
    "# # некоторые id магазинов имели разные категории. Данным магазинам присваивается наиболее встречаемая категория среди данного магазина\n",
    "# for i in tqdm(df['tail'].unique()):\n",
    "#     categories = list(df[df['tail']==i].tail_type)\n",
    "#     if categories != [categories[0]]*len(categories):\n",
    "#         dict_categories = collections.Counter(categories)\n",
    "#         max_categories = sorted(dict_categories, key=dict_categories.get, reverse=True)[0]\n",
    "#         df.loc[(df['tail'] == i), 'tail_type'] = max_categories\n",
    "\n",
    "# df.to_csv(r'data_unique_tailtype.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/eegorova/.conda/envs/ml/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3793035\n",
      "3793035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/eegorova/.conda/envs/ml/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# df:   USER --- BUYS --- SHOP\n",
    "# df2:  SHOP --- BELONGS TO --- CATEGORY\n",
    "\n",
    "df = pd.read_csv ('data_unique_tailtype.csv')\n",
    "df = df[(df.tstmp >= '2018-01-01 00:00:00+03:00') & (df.tstmp < '2019-01-01 00:00:00+03:00')]\n",
    "\n",
    "# удаление непопулярных категорий магазинов\n",
    "top_mcc = list(df.tail_type.value_counts()[:10].rename_axis('unique_values').reset_index(name='counts')['unique_values'])\n",
    "df = df[df.tail_type.isin(top_mcc)].reset_index(drop=True)\n",
    "df = df.sort_values('tail_type')\n",
    "df['category'] = df['tail_type']\n",
    "\n",
    "# удаление элементов, которых не будет в тренирововчной выбрке, но будут в тестовой\n",
    "df['tstmp_cond'] = df['tstmp'] < '2018-08-01 00:00:00+03:00'\n",
    "temp = pd.DataFrame(df['tstmp_cond'].groupby([df['tail']]).any())\n",
    "tail_del = list(temp[temp.tstmp_cond == False].index)\n",
    "df = df.query(\"tail not in @tail_del\")\n",
    "temp = pd.DataFrame(df['tstmp_cond'].groupby([df['head']]).any())\n",
    "head_del = list(temp[temp.tstmp_cond == False].index)\n",
    "df = df.query(\"head not in @head_del\")\n",
    "df.drop('tstmp_cond', inplace=True, axis=1)\n",
    "\n",
    "# присвоение уникальным id пользователей, магазинов и категорий   чисел от 0 до len(уникальных id) и создание таблицы реальных id и присвоенных индексов\n",
    "emb_to_real = pd.DataFrame(columns = ['real_id', 'index_id', 'type'])\n",
    "persons = df['head'].nunique()\n",
    "shops = df['tail'].nunique()\n",
    "emb_to_real['real_id'] = np.concatenate((df['head'].unique(),df['tail'].unique(),df['tail_type'].unique()))\n",
    "emb_to_real['index_id'] = np.concatenate((pd.factorize(df['head'].unique())[0],(pd.factorize(df['tail'].unique())[0]+ persons),(pd.factorize(df['tail_type'].unique())[0] + persons + shops)))\n",
    "emb_to_real['type'] = ['head']*len(df['head'].unique())+['tail']*len(df['tail'].unique())+['category']*len(df['tail_type'].unique())\n",
    "emb_to_real.to_csv(r'data/emb_to_real.csv', index=False)\n",
    "df['head'] = df['head'].map(emb_to_real.set_index('real_id')['index_id'])\n",
    "df['tail'] = df['tail'].map(emb_to_real.set_index('real_id')['index_id'])\n",
    "df['tail_type'] = df['tail_type'].map(emb_to_real.set_index('real_id')['index_id'])\n",
    "df['amount'] = df['relation']\n",
    "df['relation'] = 0\n",
    "\n",
    "df.to_csv(r'data/df_2.csv', index=False)\n",
    "\n",
    "# словать с типом сущности и списоком их индексов (для negative samples)\n",
    "dict_id = {}\n",
    "dict_id['person'] = list(range(0,max(df['head'])+1))\n",
    "dict_id['tail'] = list(range(min(df['tail']),max(df['tail'])+1))\n",
    "dict_id['tail_type'] = list(range(min(df['tail_type']),max(df['tail_type'])+1))\n",
    "\n",
    "df.drop('district', inplace=True, axis=1)\n",
    "df.drop('category', inplace=True, axis=1)\n",
    "df.drop('amount', inplace=True, axis=1)\n",
    "\n",
    "df2 = df.copy(deep=True)\n",
    "df2['head'] = df2['tail']\n",
    "df2['tail'] = df2['tail_type']\n",
    "df2['tstmp'] = '2018-01-01 00:00:00+03:00'\n",
    "df2['relation'] = 1\n",
    "df2.drop('tail_type', inplace=True, axis=1)\n",
    "df2 = df2.drop_duplicates()\n",
    "df.drop('tail_type', inplace=True, axis=1)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "# # создание негативных сущностей \n",
    "# neg_head --> tail      neg_tail --> head\n",
    "df['neg_head'] = [random.sample(dict_id['person'],NEG_SAMPLE_SIZE) for i in range(len(df))]\n",
    "df['neg_tail'] = [random.sample(dict_id['tail'], NEG_SAMPLE_SIZE) for i in range(len(df))]\n",
    "df2['neg_head'] = [random.sample(dict_id['tail'],NEG_SAMPLE_SIZE) for i in range(len(df2))]\n",
    "df2['neg_tail'] = [random.sample(dict_id['tail_type']*NEG_SAMPLE_SIZE,NEG_SAMPLE_SIZE) for i in range(len(df2))]\n",
    "print(len(df)+len(df2))\n",
    "df = pd.concat([df, df2], ignore_index=True)\n",
    "print(len(df))\n",
    "df.to_csv(r'data/df_100_2.csv', index=False)\n",
    "\n",
    "# разбиение на выборки по времени\n",
    "train = df[(df.tstmp >= '2018-01-01 00:00:00+03:00') & (df.tstmp < '2018-08-01 00:00:00+03:00')]\n",
    "train.drop('tstmp', inplace=True, axis=1, errors='ignore')\n",
    "valid = df[(df.tstmp >= '2018-08-01 00:00:00+03:00') & (df.tstmp < '2018-10-01 00:00:00+03:00')]\n",
    "valid.drop('tstmp', inplace=True, axis=1, errors='ignore')\n",
    "test = df[(df.tstmp >= '2018-10-01 00:00:00+03:00') & (df.tstmp < '2019-01-01 00:00:00+03:00')]\n",
    "test.drop('tstmp', inplace=True, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2145183/2145183 [01:35<00:00, 22548.46it/s]\n",
      "/data/home/eegorova/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train_count, train_true_head, train_true_tail = defaultdict(lambda: 4), defaultdict(list), defaultdict(list)\n",
    "for i in tqdm(train.index):\n",
    "    head, relation, tail = train.loc[i,'head'], train.loc[i,'relation'],  train.loc[i,'tail']\n",
    "    train_count[(head, relation)] += 1\n",
    "    train_count[(tail, -relation-1)] += 1\n",
    "    train_true_head[(relation, tail)].append(head)\n",
    "    train_true_tail[(head, relation)].append(tail)\n",
    "\n",
    "train['subsampling_weight'] = [(1/(train_count[(train.loc[i,'head'], train.loc[i,'relation'])] \n",
    "                                + train_count[(train.loc[i,'tail'], -train.loc[i,'relation']-1)]))**(1/2)\n",
    "                                                                                for i in train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(r'data/train_100_2.csv', index=False)\n",
    "valid.to_csv(r'data/valid_100_2.csv', index=False)\n",
    "test.to_csv(r'data/test_100_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # словарь с типами сущностей и списком их индексов\n",
    "# dict_id = {}\n",
    "# dict_id['person'] = list(range(0,max(df['head'])+1))\n",
    "# for tail_type in df['tail_type'].unique():\n",
    "#     dict_id[tail_type] = list(range(min(df[df.tail_type==tail_type]['tail']),(max(df[df.tail_type==tail_type]['tail'])+1)))\n",
    "# df.to_csv(r'data/df_100_2.csv', index=False)\n",
    "# df.drop('district', inplace=True, axis=1)\n",
    "    \n",
    "# # # создание негативных сущностей \n",
    "# # neg_head --> tail      neg_tail --> head\n",
    "# df['neg_head'] = [random.sample(dict_id['person'],NEG_SAMPLE_SIZE) for _ in range(len(df))]\n",
    "# df['neg_tail'] = [random.sample(dict_id[i], NEG_SAMPLE_SIZE) for i in df.tail_type]\n",
    "\n",
    "# # # траты разбиваются на 8 категорий и в дальнейшем будут характеризировать отношения\n",
    "# # df.relation = pd.qcut(df.relation, q=8, \n",
    "# # labels=[\"small\", \"medium_small\", \"medium_small_2\", 'medium_1', 'medium_2', 'medium_large_2', 'medium_large', 'large'])    \n",
    "# # # присвоение чисел отношениям\n",
    "# # dict_rel = {key: idx for idx,key in enumerate(pd.factorize(df.relation)[1].categories)}\n",
    "# # df['relation'] = df['relation'].apply(lambda x: dict_rel[x])\n",
    "# # df['relation'] = 0\n",
    "# # разбиение на выборки по времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Pykeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'data/train_100_2.csv')\n",
    "test = pd.read_csv(r'data/test_100_2.csv')\n",
    "valid = pd.read_csv(r'data/valid_100_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('subsampling_weight', inplace=True, axis=1, errors='ignore')\n",
    "train.drop('neg_head', inplace=True, axis=1, errors='ignore')\n",
    "train.drop('neg_tail', inplace=True, axis=1, errors='ignore')\n",
    "test.drop('neg_head', inplace=True, axis=1, errors='ignore')\n",
    "test.drop('neg_tail', inplace=True, axis=1, errors='ignore')\n",
    "\n",
    "train = train.reindex(columns=['head', 'relation', 'tail'])\n",
    "test = test.reindex(columns=['head', 'relation', 'tail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = test.groupby('head')['tail'].apply(set)\n",
    "all_tail = set(range(min(train['tail']), max(train['tail'])+1))\n",
    "test_neg = all_tail - test_pos\n",
    "\n",
    "test_map_ndcg = pd.DataFrame({'head': list(test_neg.index), 'relation': 0, \n",
    "                              'tail': 4046, 'negatives': [list(i) for i in test_neg],\n",
    "                              'positives': [list(i) for i in test_pos]})\n",
    "\n",
    "test_map_ndcg['all_shop']= [sorted(x[0]+x[1]) for x in test_map_ndcg[['negatives', 'positives']].values.tolist()]\n",
    "\n",
    "train['val']=1\n",
    "freq = train.pivot_table(index='head',\n",
    "                           columns='tail',\n",
    "                           values='val',\n",
    "                           fill_value=0, aggfunc=np.sum)\n",
    "train.drop('val', inplace=True, axis=1, errors='ignore')\n",
    "test_map_ndcg['scores'] = [list(freq.loc[row['head'],row['all_shop']]) for index, row in test_map_ndcg.iterrows()]\n",
    "\n",
    "test_map_ndcg.to_csv(r'data/test_map_ndcg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unknown_link = test.copy()\n",
    "train['t_h'] = train['tail'].astype(str) +'_' + train['head'].astype(str)\n",
    "test_unknown_link['t_h'] = test_unknown_link['tail'].astype(str) +'_' + test_unknown_link['head'].astype(str)\n",
    "test_unknown_link = test_unknown_link[~test_unknown_link['t_h'].isin(train['t_h'].unique().tolist())]\n",
    "train.drop('t_h', inplace=True, axis=1, errors='ignore')\n",
    "test_unknown_link.drop('t_h', inplace=True, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = test.groupby('head')['tail'].apply(set)\n",
    "all_tail = set(range(min(train['tail']), max(train['tail'])+1))\n",
    "test_neg = all_tail - test_pos\n",
    "test_pos = test_unknown_link.groupby('head')['tail'].apply(set)\n",
    "test_neg = test_neg[test_neg.index.isin(test_pos.index)]\n",
    "\n",
    "test_map_ndcg_u = pd.DataFrame({'head': list(test_neg.index), 'relation': 0, \n",
    "                              'tail': 4046, 'negatives': [list(i) for i in test_neg],\n",
    "                              'positives': [list(i) for i in test_pos]})\n",
    "\n",
    "test_map_ndcg_u['all_shop']= [sorted(x[0]+x[1]) for x in test_map_ndcg_u[['negatives', 'positives']].values.tolist()]\n",
    "\n",
    "train['val']=1\n",
    "freq = train.pivot_table(index='head',\n",
    "                           columns='tail',\n",
    "                           values='val',\n",
    "                           fill_value=0, aggfunc=np.sum)\n",
    "train.drop('val', inplace=True, axis=1, errors='ignore')\n",
    "test_map_ndcg_u['scores'] = [list(freq.loc[row['head'],row['all_shop']]) for index, row in test_map_ndcg_u.iterrows()]\n",
    "\n",
    "test_map_ndcg_u.to_csv(r'data/test_map_ndcg_u.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open('data//pykeen//train.txt', \"w\") \n",
    "\n",
    "for row in train.values:\n",
    "    row = [str(x) for x in row]\n",
    "    joined_string = \"\\t\".join(row)\n",
    "    txt_file.writelines(joined_string+'\\n')\n",
    "\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open('data//pykeen//test.txt', \"w\")\n",
    "\n",
    "for row in test.values:\n",
    "    row = [str(x) for x in row]\n",
    "    joined_string = \"\\t\".join(row)\n",
    "    txt_file.writelines(joined_string+'\\n')\n",
    "\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open('data//pykeen//test_neg.txt', \"w\")\n",
    "\n",
    "for row in test_neg.values:\n",
    "    row = [str(x) for x in row]\n",
    "    joined_string = \"\\t\".join(row)\n",
    "    txt_file.writelines(joined_string+'\\n')\n",
    "\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open('data//pykeen//test_unknown_link.txt', \"w\")\n",
    "\n",
    "for row in test_unknown_link.values:\n",
    "    row = [str(x) for x in row]\n",
    "    joined_string = \"\\t\".join(row)\n",
    "    txt_file.writelines(joined_string+'\\n')\n",
    "\n",
    "txt_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ml]",
   "language": "python",
   "name": "conda-env-.conda-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "c688d49920be7bbf78ae94a9bca1b2bf919231a3254bde77120844e21401b4fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
